# -*- coding: utf-8 -*-
"""RoBERTo Carlos

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovpgmPd7g2RCm-jhXWnpu3JRWCwZucdV

## RoBERTo Carlos - Seu Classificador de Emoções
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import re
import unicodedata
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import Markdown, display
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score


import warnings
warnings.filterwarnings('ignore')

"""## Lendo Arquivo"""

dataset = pd.read_csv("/content/drive/MyDrive/ifood/imdb-reviews-pt-br.csv.zip", usecols=['text_pt',	'sentiment'])

def evaluate(test, predict, model):
  fig, ax = plt.subplots(figsize=(5, 3))
  sns.heatmap(confusion_matrix(test, predict), annot=True, fmt="d");
  ax.set_title(f"Confusion Matrix - {model}", fontsize=24)
  ax.set_ylabel('Real', fontsize=10)
  ax.set_xlabel('Predict', fontsize=10)
  plt.show()
  print(classification_report(test,predict))
  print('ACC: ', accuracy_score(test,predict))

"""## Introducing Deep Learning"""

pip install transformers

from sklearn.model_selection import train_test_split       # for splitting dataset
from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int
from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating
from tensorflow.keras.models import Sequential     # the model
from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture
from tensorflow.keras.callbacks import ModelCheckpoint   # save model
from tensorflow.keras.models import load_model   # load saved model
import re

from transformers import BertTokenizer

def normalize(data):
  """ Normalise (normalize) unicode data in Python to remove umlauts, accents etc. """
  return unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')

def clean_str(string):
    string  = normalize(string).decode('utf-8')
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)

    cleanr = re.compile('<.*?>')

    string = re.sub(r'\d+', '', string)
    string = re.sub(cleanr, '', string)
    string = re.sub("'", '', string)
    string = re.sub(r'\W+', ' ', string)
    string = string.replace('_', '')


    return string.strip().lower()

tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')

x_data = dataset['text_pt']
y_data = dataset['sentiment']

# PRE-PROCESS REVIEW
x_data = x_data.apply(lambda row: clean_str(row))

# ENCODE SENTIMENT -> 0 & 1
y_data = y_data.replace('pos', 1)
y_data = y_data.replace('neg', 0)

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.33)

max_length = 128

x_train = x_train.apply(lambda review: tokenizer.tokenize(review)) # Tokenizando
x_test = x_test.apply(lambda review: tokenizer.tokenize(review)) # Tokenizando

tf_batch = x_train.apply(lambda review: tokenizer(review, max_length=128, padding=True, truncation=True, return_tensors='tf'))

len(y_train)

data_train = [tokenizer.encode_plus(sentence, 
                                    return_tensors='pt', 
                                    max_length=max_length, 
                                    truncation=True, 
                                    pad_to_max_length=True)["input_ids"] for sentence in x_train]
data_test = [tokenizer.encode_plus(sentence, 
                                    return_tensors='pt', 
                                    max_length=max_length, 
                                    truncation=True, 
                                    pad_to_max_length=True)["input_ids"] for sentence in x_test]

seq_train = pad_sequences(data_train, maxlen=max_length, padding='post', truncating='post')
seq_test = pad_sequences(data_test, maxlen=max_length, padding='post', truncating='post')

seq_train = []
for i in data_train:
  seq_train.append(data_train[0].cpu().detach().numpy())

seq_test = []
for i in data_test:
  seq_test.append(data_test[0].cpu().detach().numpy())

len(seq_train)

# ARCHITECTURE
EMBED_DIM = 128
LSTM_OUT = 128
model = Sequential()
model.add(Embedding(tokenizer.vocab_size, EMBED_DIM, input_length = max_length))
model.add(LSTM(LSTM_OUT, return_sequences=True))
model.add(LSTM(LSTM_OUT))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

print(model.summary())

checkpoint = ModelCheckpoint(
    '/content/drive/MyDrive/ifood/LSTM.h5',
    monitor='accuracy',
    save_best_only=True,
    verbose=1
)

len(y_train)

model.fit(seq_train, y_train, batch_size = 128, epochs = 15, callbacks=[checkpoint])

scores = model.evaluate(x_test, y_test, verbose=0)

print("Accuracy: %.2f%%" % (scores[1]*100))

y_pred = model.predict(x_test)

true = 0
for i, y in enumerate(y_test):
  if y_pred[i] >= 0.6:
     y_pred[i] = 1
  else:
    y_pred[i] = 0

  if y == y_pred[i]:
      true += 1

print('Correct Prediction: {}'.format(true))
print('Wrong Prediction: {}'.format(len(y_pred) - true))
print('Accuracy: {}'.format(true/len(y_pred)*100))

evaluate(y_test, y_pred, 'LSTM')

loaded_model = load_model('/content/drive/MyDrive/ifood/LSTM.h5')

review = 'Essa comida é muito boa'

x_data = [tokenizer.encode(sentence) for sentence in x_data]
x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')

tokenize_words = tokenizer.encode(filtered, return_tensors='pt')
tokenize_words

tokenize_words = pad_sequences(tokenize_words, maxlen=128, padding='post', truncating='post')
print(tokenize_words)

review = clean_str(review)

tokenize_words = tokenizer.tokenize(review)
tokenize_words = tokenizer.encode(tokenize_words, return_tensors='pt')
tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')
tokenize_words

result = loaded_model.predict(tokenize_words)

if result >= 0.6:
    print('positive')
else:
    print('negative')

round(result[0][0],5)

